{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class reader(object):\n",
    "    def __init__(self,config):\n",
    "        self.config = config\n",
    "        self.train_file_path  = config.train_file_path\n",
    "        self.dict_file_path = config.dict_file_path\n",
    "        self._CSV_COLUMN_DEFAULTS = [[''],[''],[1]]\n",
    "        self._CSV_CLOLUMNS = ['Context','Utterance','Label']\n",
    "        self.word_dict = {}\n",
    "        with open(self.dict_file_path, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                word, idx = l.split(' ')[0], l.split(' ')[1]\n",
    "                self.word_dict[word] = float(idx)\n",
    "        print(\"word_dict size:\", len(self.word_dict))\n",
    "        self.UNK_ID = self.word_dict['<unk>']\n",
    "        \n",
    "        kv_initializer = tf.contrib.lookup.TextFileInitializer(\n",
    "            self.dict_file_path,tf.string,0,tf.float32,1,delimiter=\" \")\n",
    "        self.lookup_table = tf.contrib.lookup.HashTable(kv_initializer,0)\n",
    "        dataset = tf.data.TextLineDataset(self.train_file_path).repeat()\n",
    "        dataset = dataset.skip(1)\n",
    "        dataset = dataset.map(self.parseCSVLine)\n",
    "        dataset = dataset.map(self.lookUpDict)\n",
    "        dataset = dataset.filter(lambda line: \n",
    "            tf.size(line['Context']) < self.config.max_sequence)\n",
    "        dataset = dataset.filter(lambda line:\n",
    "            tf.size(line['Utterance']) < self.config.max_sequence)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.config.train_batch,\n",
    "            padded_shapes={'Context':[self.config.max_sequence],\n",
    "             'Utterance':[self.config.max_sequence],\n",
    "             'Label':[1]})\n",
    "        self.iterator = dataset.make_initializable_iterator()\n",
    "        \n",
    "    def parseCSVLine(self,value):\n",
    "        columns = tf.decode_csv(value,self._CSV_COLUMN_DEFAULTS)\n",
    "        fetures = dict(zip(self._CSV_CLOLUMNS,columns))\n",
    "        return fetures\n",
    "\n",
    "    def lookUpDict(self,value):\n",
    "        value['Context'] =  self.lookup_table.lookup(\n",
    "            tf.string_split((value['Context'],\" \"))).values\n",
    "        value['Utterance'] =  self.lookup_table.lookup(\n",
    "            tf.string_split((value['Utterance'],\" \"))).values\n",
    "        value['Label'] = [tf.cast(value['Label'],tf.float32)]\n",
    "        return value\n",
    "    \n",
    "    def init_reader(self,sess):\n",
    "        sess.run(self.lookup_table.init)\n",
    "\n",
    "    def epoch_input(self):\n",
    "        return self.iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class test_reader(object):\n",
    "    def __init__(self,config):\n",
    "        self.config = config\n",
    "        self.test_file_path  = config.test_file_path\n",
    "        self.dict_file_path = config.dict_file_path\n",
    "        self._CSV_COLUMN_DEFAULTS = [[''],[''],[''],[''],['']\n",
    "                                    ,[''],[''],[''],[''],[''],['']]\n",
    "        self._CSV_CLOLUMNS = ['Context','GroundTruth','D0'\n",
    "                             ,'D1','D2','D3','D4','D5','D6'\n",
    "                             ,'D7','D8']\n",
    "        self.word_dict = {}\n",
    "        \n",
    "        with open(self.dict_file_path, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                word, idx = l.split(' ')[0], l.split(' ')[1]\n",
    "                self.word_dict[word] = int(idx)\n",
    "        print(\"word_dict size:\", len(self.word_dict))\n",
    "        self.UNK_ID = self.word_dict['<unk>']\n",
    "        \n",
    "        kv_initializer = tf.contrib.lookup.TextFileInitializer(\n",
    "            self.dict_file_path,tf.string,0,tf.float32,1,delimiter=\" \")\n",
    "        self.lookup_table = tf.contrib.lookup.HashTable(\n",
    "            kv_initializer,self.UNK_ID)\n",
    "        dataset = tf.data.TextLineDataset(self.test_file_path)\n",
    "        dataset = dataset.skip(1)\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.map(self.parseCSVLine)\n",
    "        dataset = dataset.map(self.lookUpDict)\n",
    "\n",
    "        for key in self._CSV_CLOLUMNS:\n",
    "            dataset = dataset.filter(lambda \n",
    "                        line: tf.size(line[key]) < self.config.max_sequence)\n",
    "        padded_shapes = {}\n",
    "        for key in self._CSV_CLOLUMNS:\n",
    "            padded_shapes[key]=[self.config.max_sequence]\n",
    "        dataset = dataset.padded_batch(self.config.test_batch,\n",
    "            padded_shapes=padded_shapes)\n",
    "        self.iterator = dataset.make_initializable_iterator()\n",
    "        \n",
    "    def parseCSVLine(self,value):\n",
    "        columns = tf.decode_csv(value,self._CSV_COLUMN_DEFAULTS)\n",
    "        fetures = dict(zip(self._CSV_CLOLUMNS,columns))\n",
    "        return fetures\n",
    "\n",
    "    def lookUpDict(self,value):\n",
    "        for key in value:\n",
    "            value[key] = self.lookup_table.lookup(\n",
    "                tf.string_split((value[key],\" \"))).values\n",
    "        return value\n",
    "    \n",
    "    def init_reader(self,sess):\n",
    "        sess.run(self.lookup_table.init)\n",
    "    \n",
    "    def epoch_input(self):\n",
    "        return self.iterator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dict size: 434055\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "embeddings_index = {}\n",
    "embedding_file = \"./glove.6B.100d.txt\"\n",
    "with open(embedding_file, 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except ValueError:\n",
    "            continue\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "word_dict = {}\n",
    "dict_file_path = \"./tokenizer.dict\"      \n",
    "with open(dict_file_path, 'r') as f:\n",
    "    for l in f.readlines():\n",
    "        word, idx = l.split(' ')[0], l.split(' ')[1]\n",
    "        word_dict[word] = int(idx)\n",
    "print(\"word_dict size:\", len(word_dict))\n",
    "\n",
    "MAX_NB_WORDS = len(embeddings_index)\n",
    "num_words = min(MAX_NB_WORDS, len(word_dict)) + 1\n",
    "embedding_matrix = np.zeros((num_words , 100))\n",
    "\n",
    "for word, i in word_dict.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config(object):\n",
    "    def __init__(self):\n",
    "        self.train_batch = 512\n",
    "        self.test_batch = 2\n",
    "        self.max_sequence = 160\n",
    "        self.dict_file_path = \"./tokenizer.dict\"\n",
    "        self.train_file_path = \"../data/train.csv\"\n",
    "        self.test_file_path = \"../data/test.csv\"\n",
    "        self.embedding_dim = 100\n",
    "        self.lstm_units = 200\n",
    "        self.epochs = 1\n",
    "        self.steps_per_epoch = 100\n",
    "        self.test_steps = 2\n",
    "        self.weight_path = './saved_wt.h5'\n",
    "        \n",
    "m_config = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 160)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 160)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 200)          40240900    input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1)            0           sequential_3[1][0]               \n",
      "                                                                 sequential_3[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            2           dot_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 40,240,902\n",
      "Trainable params: 40,240,902\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.6966 - acc: 0.5033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd224d1ed68>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Bidirectional\n",
    "from keras.layers import Embedding, LSTM, Input\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.layers import merge\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "\n",
    "train_xc_textline = reader(m_config)\n",
    "train_xr_textline = reader(m_config)\n",
    "train_xl_textline = reader(m_config)\n",
    "train_xc_textline.init_reader(K.get_session())\n",
    "train_xr_textline.init_reader(K.get_session())\n",
    "train_xl_textline.init_reader(K.get_session())\n",
    "K.get_session().run(train_xc_textline.epoch_input().initializer)\n",
    "K.get_session().run(train_xr_textline.epoch_input().initializer)\n",
    "K.get_session().run(train_xl_textline.epoch_input().initializer)\n",
    "train_c = train_xc_textline.epoch_input().get_next()['Context']\n",
    "train_r = train_xr_textline.epoch_input().get_next()['Utterance']\n",
    "train_l = train_xl_textline.epoch_input().get_next()['Label']\n",
    "encoder = Sequential()\n",
    "embeddin_layer = Embedding(output_dim=m_config.embedding_dim,\n",
    "                        input_dim=MAX_NB_WORDS+1,\n",
    "                        input_length=m_config.max_sequence,\n",
    "                        weights=[embedding_matrix],\n",
    "                        mask_zero=True,\n",
    "                        trainable=True)\n",
    "lstm_layer = LSTM(units=m_config.lstm_units)\n",
    "encoder.add(embeddin_layer)\n",
    "encoder.add(lstm_layer)\n",
    "\n",
    "context_input = Input(shape=(m_config.max_sequence,),tensor=train_c)\n",
    "response_input = Input(shape=(m_config.max_sequence,),tensor=train_r)\n",
    "\n",
    "context_branch = encoder(context_input)\n",
    "response_branch = encoder(response_input)\n",
    "\n",
    "concatenated = keras.layers.Dot(axes=1)([context_branch, response_branch])\n",
    "out = Dense((1), activation = \"sigmoid\") (concatenated)\n",
    "dual_encoder = Model([context_input, response_input], out)\n",
    "dual_encoder.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'],\n",
    "                target_tensors = [train_l])\n",
    "dual_encoder.summary()\n",
    "dual_encoder.fit(epochs = m_config.epochs,\n",
    "                 steps_per_epoch = m_config.steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(key,config):\n",
    "    K.clear_session()\n",
    "    test_xc_textline = test_reader(config)\n",
    "    test_xr_textline = test_reader(config)\n",
    "    test_xc_textline.init_reader(K.get_session())\n",
    "    test_xr_textline.init_reader(K.get_session())\n",
    "    K.get_session().run(test_xc_textline.epoch_input().initializer)\n",
    "    K.get_session().run(test_xr_textline.epoch_input().initializer)\n",
    "    test_c = test_xc_textline.epoch_input().get_next()['Context']\n",
    "    test_r = test_xr_textline.epoch_input().get_next()[key]\n",
    "    \n",
    "    \n",
    "    test_encoder = Sequential()\n",
    "    embeddin_layer = Embedding(output_dim=config.embedding_dim,\n",
    "                            input_dim=MAX_NB_WORDS+1,\n",
    "                            input_length=config.max_sequence,\n",
    "                            weights=[embedding_matrix],\n",
    "                            mask_zero=True,\n",
    "                            trainable=True)\n",
    "    lstm_layer = LSTM(units=config.lstm_units)\n",
    "    test_encoder.add(embeddin_layer)\n",
    "    test_encoder.add(lstm_layer)\n",
    "\n",
    "    test_context_input = Input(shape=(config.max_sequence,),tensor=test_c)\n",
    "    test_response_input = Input(shape=(config.max_sequence,),tensor=test_r)\n",
    "    # context_input = Input(shape=(160,), dtype='float32')\n",
    "    # response_input = Input(shape=(160,), dtype='float32')\n",
    "\n",
    "    test_context_branch = test_encoder(test_context_input)\n",
    "    test_response_branch = test_encoder(test_response_input)\n",
    "\n",
    "    test_concatenated = keras.layers.Dot(axes=1)(\n",
    "                                [test_context_branch, \n",
    "                                 test_response_branch])\n",
    "    test_out = Dense((1), activation = \"sigmoid\") (test_concatenated)\n",
    "    test_dual_encoder = Model([test_context_input, test_response_input], test_out)\n",
    "    test_dual_encoder.load_weights(config.weight_path)\n",
    "    test_dual_encoder.compile(loss='binary_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "#     test_dual_encoder.summary()\n",
    "\n",
    "    his = test_dual_encoder.predict(x=None,steps=config.test_steps)\n",
    "    return his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D0\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D1\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D2\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D3\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D4\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D5\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D6\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D7\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D8\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "key_list =  ['GroundTruth','D0','D1','D2','D3','D4','D5','D6','D7','D8']\n",
    "res = []\n",
    "for key in key_list:\n",
    "    print(key)\n",
    "    res.append(test_model(key,m_config))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "res = np.array(res)\n",
    "count = 0\n",
    "for i in range(len(res[0])):\n",
    "    if 0 in res[:,i].reshape(10).argsort()[-8:]:\n",
    "        count+=1\n",
    "print(count/(m_config.test_batch*m_config.test_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
