{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunzewen/first-week/first-week/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/sunzewen/first-week/first-week/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "train_df.Label = train_df.Label.astype('category')\n",
    "\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "validation_df = pd.read_csv(\"../data/valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>957097</td>\n",
       "      <td>736145</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>! op __eou__ __eot__ ? __eou__ __eot__</td>\n",
       "      <td>thank __eou__</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>15</td>\n",
       "      <td>12426</td>\n",
       "      <td>500127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Context      Utterance    Label\n",
       "count                                  1000000        1000000  1000000\n",
       "unique                                  957097         736145        2\n",
       "top     ! op __eou__ __eot__ ? __eou__ __eot__  thank __eou__        0\n",
       "freq                                        15          12426   500127"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i think we could import the old comment via rsync , but from there we need to go via email . i think it be easier than cach the status on each bug and than import bite here and there __eou__ __eot__ it would be veri easi to keep a hash db of message-id __eou__ sound good __eou__ __eot__ ok __eou__ perhap we can ship an ad-hoc apt_preferec __eou__ __eot__ version ? __eou__ __eot__ thank __eou__ __eot__ not yet __eou__ it be cover by your insur ? __eou__ __eot__ yes __eou__ but it 's realli no...</td>\n",
       "      <td>basic each xfree86 upload will not forc user to upgrad 100mb of font for noth __eou__ no someth i do in my spare time . __eou__</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i 'm not suggest all - onli the one you modifi . __eou__ __eot__ ok , it sound like you re agre with me , then __eou__ though rather than `` the one we modifi '' , my idea be `` the one we need to merg '' __eou__ __eot__</td>\n",
       "      <td>sorri __eou__ i think it be ubuntu relat . __eou__</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afternoon all __eou__ not entir relat to warti , but if grub-instal take 5 minut to instal , be this a sign that i should just retri the instal : ) __eou__ __eot__ here __eou__ __eot__ you might want to know that thinic in warti be buggi compar to that in sid __eou__ __eot__ and appar gnome be suddent almost perfect ( out of the thinic problem ) , nobodi report bug : -p __eou__ i do n't get your question , where do you want to past ? __eou__ __eot__ can i file the panel not link to ed ? : ) ...</td>\n",
       "      <td>yep . __eou__ oh , okay . i wonder what happen to you __eou__ what distro do you need ? __eou__ yes __eou__</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interest __eou__ grub-instal work with / be ext3 , fail when it be xfs __eou__ i think d-i instal the relev kernel for your machin . i have a p4 and it instal the 386 kernel __eou__ holi crap a lot of stuff get instal by default : ) __eou__ you be instal vim on a box of mine __eou__ ; ) __eou__ __eot__ more like osx than debian ; ) __eou__ we have a select of python modul avail for great justic ( and python develop ) __eou__ __eot__ 2.8 be fix them iirc __eou__ __eot__ pong __eou__ vino will...</td>\n",
       "      <td>that the one __eou__</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and becaus python give mark a woodi __eou__ __eot__ i 'm not sure if we re mean to talk about that public yet . __eou__ __eot__ and i think we be a `` pant off '' kind of compani ... : p __eou__ you need new glass __eou__ __eot__ mono 1.0 ? dude , that 's go to be a barrel of laugh for total non-releas relat reason dure hoari __eou__ read bryan clark 's entri about networkmanag ? __eou__ __eot__ there be an accompani irc convers to that one &lt; g &gt; __eou__ explain ? __eou__ i guess you could s...</td>\n",
       "      <td>( i think someon be go to make a joke about .au bandwidth ... ) __eou__ especi not if you re use screen ; ) __eou__</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Context  \\\n",
       "0  i think we could import the old comment via rsync , but from there we need to go via email . i think it be easier than cach the status on each bug and than import bite here and there __eou__ __eot__ it would be veri easi to keep a hash db of message-id __eou__ sound good __eou__ __eot__ ok __eou__ perhap we can ship an ad-hoc apt_preferec __eou__ __eot__ version ? __eou__ __eot__ thank __eou__ __eot__ not yet __eou__ it be cover by your insur ? __eou__ __eot__ yes __eou__ but it 's realli no...   \n",
       "1                                                                                                                                                                                                                                                                                         i 'm not suggest all - onli the one you modifi . __eou__ __eot__ ok , it sound like you re agre with me , then __eou__ though rather than `` the one we modifi '' , my idea be `` the one we need to merg '' __eou__ __eot__   \n",
       "2  afternoon all __eou__ not entir relat to warti , but if grub-instal take 5 minut to instal , be this a sign that i should just retri the instal : ) __eou__ __eot__ here __eou__ __eot__ you might want to know that thinic in warti be buggi compar to that in sid __eou__ __eot__ and appar gnome be suddent almost perfect ( out of the thinic problem ) , nobodi report bug : -p __eou__ i do n't get your question , where do you want to past ? __eou__ __eot__ can i file the panel not link to ed ? : ) ...   \n",
       "3  interest __eou__ grub-instal work with / be ext3 , fail when it be xfs __eou__ i think d-i instal the relev kernel for your machin . i have a p4 and it instal the 386 kernel __eou__ holi crap a lot of stuff get instal by default : ) __eou__ you be instal vim on a box of mine __eou__ ; ) __eou__ __eot__ more like osx than debian ; ) __eou__ we have a select of python modul avail for great justic ( and python develop ) __eou__ __eot__ 2.8 be fix them iirc __eou__ __eot__ pong __eou__ vino will...   \n",
       "4  and becaus python give mark a woodi __eou__ __eot__ i 'm not sure if we re mean to talk about that public yet . __eou__ __eot__ and i think we be a `` pant off '' kind of compani ... : p __eou__ you need new glass __eou__ __eot__ mono 1.0 ? dude , that 's go to be a barrel of laugh for total non-releas relat reason dure hoari __eou__ read bryan clark 's entri about networkmanag ? __eou__ __eot__ there be an accompani irc convers to that one < g > __eou__ explain ? __eou__ i guess you could s...   \n",
       "\n",
       "                                                                                                                         Utterance  \\\n",
       "0  basic each xfree86 upload will not forc user to upgrad 100mb of font for noth __eou__ no someth i do in my spare time . __eou__   \n",
       "1                                                                               sorri __eou__ i think it be ubuntu relat . __eou__   \n",
       "2                      yep . __eou__ oh , okay . i wonder what happen to you __eou__ what distro do you need ? __eou__ yes __eou__   \n",
       "3                                                                                                             that the one __eou__   \n",
       "4              ( i think someon be go to make a joke about .au bandwidth ... ) __eou__ especi not if you re use screen ; ) __eou__   \n",
       "\n",
       "  Label  \n",
       "0     1  \n",
       "1     0  \n",
       "2     0  \n",
       "3     1  \n",
       "4     1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth=500\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7efd686dd048>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.Label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_context_len = train_df.Context.str.split(\" \").apply(len)\n",
    "train_df_Utterance_len = train_df.Utterance.str.split(\" \").apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1000000.000000\n",
      "mean          86.339195\n",
      "std           74.929713\n",
      "min            5.000000\n",
      "25%           37.000000\n",
      "50%           63.000000\n",
      "75%          108.000000\n",
      "max         1879.000000\n",
      "Name: Context, dtype: float64\n",
      "count    1000000.000000\n",
      "mean          17.246392\n",
      "std           16.422901\n",
      "min            1.000000\n",
      "25%            7.000000\n",
      "50%           13.000000\n",
      "75%           22.000000\n",
      "max          653.000000\n",
      "Name: Utterance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train_df_context_len.describe())\n",
    "print(train_df_Utterance_len.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Ground Truth Utterance</th>\n",
       "      <th>Distractor_0</th>\n",
       "      <th>Distractor_1</th>\n",
       "      <th>Distractor_2</th>\n",
       "      <th>Distractor_3</th>\n",
       "      <th>Distractor_4</th>\n",
       "      <th>Distractor_5</th>\n",
       "      <th>Distractor_6</th>\n",
       "      <th>Distractor_7</th>\n",
       "      <th>Distractor_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18920</td>\n",
       "      <td>18920</td>\n",
       "      <td>18920</td>\n",
       "      <td>18920</td>\n",
       "      <td>18920</td>\n",
       "      <td>18920</td>\n",
       "      <td>18920</td>\n",
       "      <td>18920</td>\n",
       "      <td>18920</td>\n",
       "      <td>18920</td>\n",
       "      <td>18920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>18920</td>\n",
       "      <td>17914</td>\n",
       "      <td>13982</td>\n",
       "      <td>13902</td>\n",
       "      <td>14077</td>\n",
       "      <td>14041</td>\n",
       "      <td>14101</td>\n",
       "      <td>14072</td>\n",
       "      <td>13969</td>\n",
       "      <td>13975</td>\n",
       "      <td>14123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ubiqu be the name of the instal , exact the sa...</td>\n",
       "      <td>thank __eou__</td>\n",
       "      <td>thank __eou__</td>\n",
       "      <td>thank __eou__</td>\n",
       "      <td>thank __eou__</td>\n",
       "      <td>thank __eou__</td>\n",
       "      <td>thank __eou__</td>\n",
       "      <td>thank __eou__</td>\n",
       "      <td>thank __eou__</td>\n",
       "      <td>thank __eou__</td>\n",
       "      <td>thank __eou__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>186</td>\n",
       "      <td>176</td>\n",
       "      <td>186</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "      <td>167</td>\n",
       "      <td>197</td>\n",
       "      <td>190</td>\n",
       "      <td>188</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Context  \\\n",
       "count                                               18920   \n",
       "unique                                              18920   \n",
       "top     ubiqu be the name of the instal , exact the sa...   \n",
       "freq                                                    1   \n",
       "\n",
       "       Ground Truth Utterance   Distractor_0   Distractor_1   Distractor_2  \\\n",
       "count                   18920          18920          18920          18920   \n",
       "unique                  17914          13982          13902          14077   \n",
       "top             thank __eou__  thank __eou__  thank __eou__  thank __eou__   \n",
       "freq                      186            176            186            194   \n",
       "\n",
       "         Distractor_3   Distractor_4   Distractor_5   Distractor_6  \\\n",
       "count           18920          18920          18920          18920   \n",
       "unique          14041          14101          14072          13969   \n",
       "top     thank __eou__  thank __eou__  thank __eou__  thank __eou__   \n",
       "freq              195            167            197            190   \n",
       "\n",
       "         Distractor_7   Distractor_8  \n",
       "count           18920          18920  \n",
       "unique          13975          14123  \n",
       "top     thank __eou__  thank __eou__  \n",
       "freq              188            201  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth=50\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunzewen/first-week/first-week/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dict size: 15003\n",
      "{'lenResponse': array([10, 25,  5,  8, 13, 14, 10, 23, 21, 11, 25, 29, 29,  5, 10,  2, 18,\n",
      "        8, 11, 17, 16,  8, 14, 17, 69, 38, 45,  3,  5, 23, 42, 21],\n",
      "      dtype=int32), 'Utterance': array([[    0.,     0.,  2341., ...,     0.,     0.,     0.],\n",
      "       [ 3856., 10723.,  4133., ...,     0.,     0.,     0.],\n",
      "       [ 8511.,  7469.,     0., ...,     0.,     0.,     0.],\n",
      "       ...,\n",
      "       [    0.,  4444.,  8938., ...,     0.,     0.,     0.],\n",
      "       [    0.,     0.,  7475., ...,     0.,     0.,     0.],\n",
      "       [ 2341.,     0.,  2341., ...,     0.,     0.,     0.]],\n",
      "      dtype=float32), 'Context': array([[ 2341.,     0.,  6658., ...,     0.,     0.,     0.],\n",
      "       [    0.,  7062., 12439., ...,     0.,     0.,     0.],\n",
      "       [ 3687.,  2032.,     0., ...,     0.,     0.,     0.],\n",
      "       ...,\n",
      "       [    0.,   115.,     0., ...,     0.,     0.,     0.],\n",
      "       [ 4479.,   115.,     0., ...,     0.,     0.,     0.],\n",
      "       [ 6363.,  3687.,     0., ...,     0.,     0.,     0.]],\n",
      "      dtype=float32), 'lenQuery': array([50, 46, 70, 89, 27, 68, 44, 46, 42, 27, 19, 80, 90, 73, 68, 46, 38,\n",
      "       36, 45, 81, 45, 66, 22, 93, 97, 16, 28, 99, 93, 16, 98, 59],\n",
      "      dtype=int32), 'Label': array([0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "class reader(object):\n",
    "    def __init__(self):\n",
    "        self.train_file_path  = \"../data/train.csv\"\n",
    "        self.dict_file_path = \"/mnt/cephfs/dataset/sentence_with_punc/ch_en_15k.dict\"\n",
    "        self._CSV_COLUMN_DEFAULTS = [[''],[''],[1]]\n",
    "        self._CSV_CLOLUMNS = ['Context','Utterance','Label']\n",
    "        self.word_dict = {}\n",
    "        with open(self.dict_file_path, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                word, idx = l.split(' ')[0], l.split(' ')[1]\n",
    "                self.word_dict[word] = int(idx)\n",
    "        print(\"word_dict size:\", len(self.word_dict))\n",
    "        self.UNK_ID = self.word_dict['<unk>']\n",
    "        \n",
    "        \n",
    "        kv_initializer = tf.contrib.lookup.TextFileInitializer(\n",
    "            self.dict_file_path,tf.string,0,tf.float32,1,delimiter=\" \")\n",
    "        self.lookup_table = tf.contrib.lookup.HashTable(kv_initializer,self.UNK_ID)\n",
    "        dataset = tf.data.TextLineDataset(self.train_file_path).repeat()\n",
    "        dataset = dataset.skip(1)\n",
    "        dataset = dataset.map(self.parseCSVLine)\n",
    "        dataset = dataset.map(self.lookUpDict)\n",
    "        dataset = dataset.filter(lambda line: line['lenQuery']<100)\n",
    "        dataset = dataset.filter(lambda line: line['lenResponse']<100)\n",
    "#         filter(lambda line: tf.not_equal(tf.substr(line, 0, 1), \"#\")\n",
    "        dataset = dataset.padded_batch(32,padded_shapes={'Context':[100],\n",
    "                                                         'Utterance':[100],\n",
    "                                                         'Label':[],\n",
    "                                                         'lenQuery':[],\n",
    "                                                         'lenResponse':[]})\n",
    "#         dataset = dataset.batch(10)\n",
    "        self.iterator = dataset.make_initializable_iterator()\n",
    "        \n",
    "    def parseCSVLine(self,value):\n",
    "        columns = tf.decode_csv(value,self._CSV_COLUMN_DEFAULTS)\n",
    "        fetures = dict(zip(self._CSV_CLOLUMNS,columns))\n",
    "        return fetures\n",
    "\n",
    "    def lookUpDict(self,value):\n",
    "#         print(value['Query'])\n",
    "#         value['Query'] = self.lookup_table.lookup(value['Context'])\n",
    "        value['Context'] = self.lookup_table.lookup(\n",
    "            tf.string_split((value['Context'],\" \"))).values\n",
    "        value['lenQuery'] = tf.size(value['Context'])\n",
    "        value['Utterance'] = self.lookup_table.lookup(\n",
    "            tf.string_split((value['Utterance'],\" \"))).values\n",
    "        value['lenResponse'] = tf.size(value['Utterance'])\n",
    "        value['Label'] = tf.cast(value['Label'],tf.float32)\n",
    "        return value\n",
    "    \n",
    "    def init_reader(self,sess):\n",
    "        sess.run(self.lookup_table.init)\n",
    "    \n",
    "    def epoch_input(self):\n",
    "        return self.iterator\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_textline = reader()\n",
    "    train_textline.init_reader(sess)\n",
    "    sess.run(train_textline.epoch_input().initializer)\n",
    "    for i in range(1):\n",
    "        a = train_textline.epoch_input().get_next()['Context']\n",
    "        print(sess.run(train_textline.epoch_input().get_next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dict size: 15003\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 30)           8120        input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1)            0           sequential_3[1][0]               \n",
      "                                                                 sequential_3[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            2           dot_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 8,122\n",
      "Trainable params: 8,122\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      " 4175/10000 [===========>..................] - ETA: 45:28 - loss: 0.6932 - acc: 0.4996"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Bidirectional\n",
    "from keras.layers import Embedding, LSTM, Input\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.layers import merge\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "\n",
    "train_textline = reader()\n",
    "train_textline.init_reader(K.get_session())\n",
    "K.get_session().run(train_textline.epoch_input().initializer)\n",
    "\n",
    "encoder = Sequential()\n",
    "embeddin_layer = Embedding(output_dim=20,input_dim=100,\n",
    "                      input_length=400,mask_zero=True,trainable=True)\n",
    "encoder.add(embeddin_layer)\n",
    "encoder.add(LSTM(units=30))\n",
    "\n",
    "# context_input = Input(tensor=tf.reshape(train_textline.epoch_input().get_next()['Context'],[-1,1]))\n",
    "# response_input = Input(tensor=tf.reshape(train_textline.epoch_input().get_next()['Utterance'],[-1,1]))\n",
    "context_input = Input(tensor=train_textline.epoch_input().get_next()['Context'])\n",
    "response_input = Input(tensor=train_textline.epoch_input().get_next()['Utterance'])\n",
    "context_branch = encoder(context_input)\n",
    "response_branch = encoder(response_input)\n",
    "# concatenated = keras.layers.Multiply()([context_branch, response_branch])\n",
    "\n",
    "concatenated = keras.layers.Dot(axes=1)([context_branch, response_branch])\n",
    "out = Dense((1), activation = \"sigmoid\") (concatenated)\n",
    "dual_encoder = Model([context_input, response_input], out)\n",
    "# dual_encoder = multi_gpu_model(dual_encoder,gpus=2)\n",
    "dual_encoder.compile(loss='binary_crossentropy',\n",
    "                optimizer='SGD',\n",
    "                metrics=['accuracy'],\n",
    "                target_tensors = [tf.reshape(train_textline.epoch_input().get_next()['Label'],[-1,1])])\n",
    "dual_encoder.summary()\n",
    "dual_encoder.fit(epochs=1,steps_per_epoch=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
