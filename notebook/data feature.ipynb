{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Data\n",
    "# train_df = pd.read_csv(\"../data/train.csv\")\n",
    "# train_df.Label = train_df.Label.astype('category')\n",
    "\n",
    "# test_df = pd.read_csv(\"../data/test.csv\")\n",
    "# validation_df = pd.read_csv(\"../data/valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.max_colwidth=500\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.Label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_context_len = train_df.Context.str.split(\" \").apply(len)\n",
    "# train_df_Utterance_len = train_df.Utterance.str.split(\" \").apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_df_context_len.describe()['max'])\n",
    "# print(train_df_Utterance_len.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.max_colwidth=50\n",
    "# test_df.head(1)\n",
    "# test_df.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df_context_len = test_df['Context'].str.split(\" \").apply(len)\n",
    "# print(test_df_context_len.describe()['max'])\n",
    "\n",
    "# for key in test_df.columns.values.tolist():\n",
    "#     a = test_df[key].str.split(\" \").apply(len)\n",
    "#     print(a.describe()['max'])\n",
    "\n",
    "\n",
    "\n",
    "# test_df_context_len = train_df.Distractor_0.str.split(\" \").apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "# tokenizer = Tokenizer()\n",
    "# train_file = open(\"../data/train.csv\",'r')\n",
    "# tokenizer.fit_on_texts(train_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = open(\"../data/test.csv\",'r')\n",
    "# valid_file = open(\"../data/valid.csv\",'r')\n",
    "# tokenizer.fit_on_texts(test_file)\n",
    "# tokenizer.fit_on_texts(valid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tokenizer.word_index))\n",
    "# with tf.Session() as sess:\n",
    "# #     a = tokenizer.texts_to_sequences(\n",
    "# #         tf.string_split((\"hello world\",\" \")).values)\n",
    "#     a = tf.string_split((\"hello world\",\" \")).values\n",
    "#     print(sess.run(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/mnt/cephfs/dataset/sentence_with_punc/ch_en_15k.dict\",'r') as f:\n",
    "#     print(f.readlines()[0:10])\n",
    "\n",
    "# with open(\"./tokenizer.dict\",'w') as f:\n",
    "#     count = 0\n",
    "#     f.write('<unk>'+\" \"+str(0)+'\\n')\n",
    "#     for word,i in tokenizer.word_index.items():\n",
    "#         count += 1\n",
    "#         f.write(word+\" \"+str(i)+'\\n')\n",
    "#         if count%1000 == 0:\n",
    "#             print(count/434054*100.0)\n",
    "        \n",
    "# with open(\"./tokenizer.dict\",'r') as f:\n",
    "\n",
    "#     print(f.readlines()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class reader(object):\n",
    "    def __init__(self):\n",
    "        self.train_file_path  = \"../data/train.csv\"\n",
    "        self.dict_file_path = \"./tokenizer.dict\"\n",
    "        self._CSV_COLUMN_DEFAULTS = [[''],[''],[1]]\n",
    "        self._CSV_CLOLUMNS = ['Context','Utterance','Label']\n",
    "        self.word_dict = {}\n",
    "        with open(self.dict_file_path, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                word, idx = l.split(' ')[0], l.split(' ')[1]\n",
    "                self.word_dict[word] = float(idx)\n",
    "        print(\"word_dict size:\", len(self.word_dict))\n",
    "        self.UNK_ID = self.word_dict['<unk>']\n",
    "        \n",
    "        \n",
    "        kv_initializer = tf.contrib.lookup.TextFileInitializer(\n",
    "            self.dict_file_path,tf.string,0,tf.float32,1,delimiter=\" \")\n",
    "        self.lookup_table = tf.contrib.lookup.HashTable(kv_initializer,0)\n",
    "        dataset = tf.data.TextLineDataset(self.train_file_path).repeat()\n",
    "        dataset = dataset.skip(1)\n",
    "        dataset = dataset.map(self.parseCSVLine)\n",
    "        dataset = dataset.map(self.lookUpDict)\n",
    "        \n",
    "        \n",
    "        dataset = dataset.filter(lambda line: line['lenQuery']<160)\n",
    "        dataset = dataset.filter(lambda line: line['lenResponse']<160)\n",
    "#         filter(lambda line: tf.not_equal(tf.substr(line, 0, 1), \"#\")\n",
    "        dataset = dataset.padded_batch(512,padded_shapes={'Context':[160],\n",
    "                                                         'Utterance':[160],\n",
    "                                                         'Label':[1],\n",
    "                                                         'lenQuery':[],\n",
    "                                                         'lenResponse':[]})\n",
    "#         dataset = dataset.batch(10)\n",
    "        self.iterator = dataset.make_initializable_iterator()\n",
    "        \n",
    "    def parseCSVLine(self,value):\n",
    "        columns = tf.decode_csv(value,self._CSV_COLUMN_DEFAULTS)\n",
    "        fetures = dict(zip(self._CSV_CLOLUMNS,columns))\n",
    "        return fetures\n",
    "\n",
    "    def lookUpDict(self,value):\n",
    "#         print(value['Query'])\n",
    "#         value['Query'] = self.lookup_table.lookup(value['Context'])\n",
    "\n",
    "        value['Context'] =  self.lookup_table.lookup(\n",
    "            tf.string_split((value['Context'],\" \"))).values\n",
    "        value['lenQuery'] = tf.size(value['Context'])\n",
    "        value['Utterance'] =  self.lookup_table.lookup(\n",
    "            tf.string_split((value['Utterance'],\" \"))).values\n",
    "        value['lenResponse'] = tf.size(value['Utterance'])\n",
    "        value['Label'] = [tf.cast(value['Label'],tf.float32)]\n",
    "        return value\n",
    "    \n",
    "    def init_reader(self,sess):\n",
    "        sess.run(self.lookup_table.init)\n",
    "    \n",
    "    def epoch_input(self):\n",
    "        return self.iterator\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     train_textline = reader()\n",
    "#     train_textline.init_reader(sess)\n",
    "#     sess.run(train_textline.epoch_input().initializer)\n",
    "#     a = sess.run(train_textline.epoch_input().get_next()['Label'])\n",
    "#     for i in range(2):\n",
    "#         print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class test_reader(object):\n",
    "    def __init__(self):\n",
    "        self.test_file_path  = \"../data/test.csv\"\n",
    "        self.dict_file_path = \"./tokenizer.dict\"\n",
    "        self._CSV_COLUMN_DEFAULTS = [[''],[''],[''],[''],['']\n",
    "                                    ,[''],[''],[''],[''],[''],['']]\n",
    "        self._CSV_CLOLUMNS = ['Context','GroundTruth','D0'\n",
    "                             ,'D1','D2','D3','D4','D5','D6'\n",
    "                             ,'D7','D8']\n",
    "        self.word_dict = {}\n",
    "        \n",
    "        with open(self.dict_file_path, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                word, idx = l.split(' ')[0], l.split(' ')[1]\n",
    "                self.word_dict[word] = int(idx)\n",
    "        print(\"word_dict size:\", len(self.word_dict))\n",
    "        self.UNK_ID = self.word_dict['<unk>']\n",
    "        \n",
    "        kv_initializer = tf.contrib.lookup.TextFileInitializer(\n",
    "            self.dict_file_path,tf.string,0,tf.float32,1,delimiter=\" \")\n",
    "        self.lookup_table = tf.contrib.lookup.HashTable(kv_initializer,self.UNK_ID)\n",
    "        dataset = tf.data.TextLineDataset(self.test_file_path)\n",
    "        dataset = dataset.skip(1)\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.map(self.parseCSVLine)\n",
    "        dataset = dataset.map(self.lookUpDict)\n",
    "\n",
    "        for key in self._CSV_CLOLUMNS:\n",
    "            dataset = dataset.filter(lambda line: tf.size(line[key])<150)\n",
    "        padded_shapes = {}\n",
    "        for key in self._CSV_CLOLUMNS:\n",
    "            padded_shapes[key]=[150]\n",
    "        dataset = dataset.padded_batch(512,padded_shapes=padded_shapes)\n",
    "        dataset =  dataset.map(self.get2together)\n",
    "        self.iterator = dataset.make_initializable_iterator()\n",
    "    \n",
    "    def get2together(self, value):\n",
    "#         value['Compare'] = []\n",
    "#         for key in self._CSV_CLOLUMNS:\n",
    "#             if key!='Context':\n",
    "#                 value['Compare'].append([value['Context'],value[key]])\n",
    "#         for key in self._CSV_CLOLUMNS:\n",
    "#             value.pop(key)\n",
    "#         return value\n",
    "        \n",
    "#         value['Compare'] = []\n",
    "#         for i in range(1):\n",
    "#             value['Compare'].append([value['Context'][i],value['GroundTruth'][i]])\n",
    "#         for key in self._CSV_CLOLUMNS:\n",
    "#             value.pop(key)\n",
    "        return value\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def parseCSVLine(self,value):\n",
    "        columns = tf.decode_csv(value,self._CSV_COLUMN_DEFAULTS)\n",
    "        fetures = dict(zip(self._CSV_CLOLUMNS,columns))\n",
    "        return fetures\n",
    "\n",
    "    def lookUpDict(self,value):\n",
    "        for key in value:\n",
    "            value[key] = self.lookup_table.lookup(\n",
    "                tf.string_split((value[key],\" \"))).values\n",
    "\n",
    "        return value\n",
    "    \n",
    "    def init_reader(self,sess):\n",
    "        sess.run(self.lookup_table.init)\n",
    "    \n",
    "    def epoch_input(self):\n",
    "        return self.iterator\n",
    "    \n",
    "# with tf.Session() as sess:\n",
    "#     test_textline = test_reader()\n",
    "#     test_textline.init_reader(sess)\n",
    "#     sess.run(test_textline.epoch_input().initializer)\n",
    "#     for i in range(1):\n",
    "#         print(sess.run(test_textline.epoch_input().get_next()['GroundTruth']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dict size: 434055\n"
     ]
    }
   ],
   "source": [
    "#embedding\n",
    "import numpy as np\n",
    "embeddings_index = {}\n",
    "embedding_file = \"./glove.6B.100d.txt\"\n",
    "with open(embedding_file, 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except ValueError:\n",
    "            continue\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "word_dict = {}\n",
    "dict_file_path = \"./tokenizer.dict\"      \n",
    "with open(dict_file_path, 'r') as f:\n",
    "    for l in f.readlines():\n",
    "        word, idx = l.split(' ')[0], l.split(' ')[1]\n",
    "        word_dict[word] = int(idx)\n",
    "print(\"word_dict size:\", len(word_dict))\n",
    "\n",
    "MAX_NB_WORDS = len(embeddings_index)\n",
    "num_words = min(MAX_NB_WORDS, len(word_dict)) + 1\n",
    "embedding_matrix = np.zeros((num_words , 100))\n",
    "\n",
    "for word, i in word_dict.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 160)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 160)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 200)          40240900    input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1)            0           sequential_3[1][0]               \n",
      "                                                                 sequential_3[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            2           dot_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 40,240,902\n",
      "Trainable params: 40,240,902\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 898s 898ms/step - loss: 0.6373 - acc: 0.6212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f719b32f390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Bidirectional\n",
    "from keras.layers import Embedding, LSTM, Input\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.layers import merge\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "\n",
    "train_xc_textline = reader()\n",
    "train_xr_textline = reader()\n",
    "train_xl_textline = reader()\n",
    "train_xc_textline.init_reader(K.get_session())\n",
    "train_xr_textline.init_reader(K.get_session())\n",
    "train_xl_textline.init_reader(K.get_session())\n",
    "K.get_session().run(train_xc_textline.epoch_input().initializer)\n",
    "K.get_session().run(train_xr_textline.epoch_input().initializer)\n",
    "K.get_session().run(train_xl_textline.epoch_input().initializer)\n",
    "train_c = train_xc_textline.epoch_input().get_next()['Context']\n",
    "train_r = train_xr_textline.epoch_input().get_next()['Utterance']\n",
    "train_l = train_xl_textline.epoch_input().get_next()['Label']\n",
    "encoder = Sequential()\n",
    "embeddin_layer = Embedding(output_dim=100,\n",
    "                        input_dim=MAX_NB_WORDS+1,\n",
    "                        input_length=160,\n",
    "                        weights=[embedding_matrix],\n",
    "                        mask_zero=True,\n",
    "                        trainable=True)\n",
    "lstm_layer = LSTM(units=200)\n",
    "encoder.add(embeddin_layer)\n",
    "encoder.add(lstm_layer)\n",
    "\n",
    "context_input = Input(shape=(160,),tensor=train_c)\n",
    "response_input = Input(shape=(160,),tensor=train_r)\n",
    "# context_input = Input(shape=(160,), dtype='float32')\n",
    "# response_input = Input(shape=(160,), dtype='float32')\n",
    "\n",
    "context_branch = encoder(context_input)\n",
    "response_branch = encoder(response_input)\n",
    "\n",
    "concatenated = keras.layers.Dot(axes=1)([context_branch, response_branch])\n",
    "out = Dense((1), activation = \"sigmoid\") (concatenated)\n",
    "dual_encoder = Model([context_input, response_input], out)\n",
    "# dual_encoder = multi_gpu_model(dual_encoder,gpus=1)\n",
    "sgd = SGD(lr=0.1,momentum=0.1,decay=0.1,nesterov=False)\n",
    "dual_encoder.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'],\n",
    "                target_tensors = [train_l])\n",
    "dual_encoder.summary()\n",
    "dual_encoder.fit(epochs=1,steps_per_epoch=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 160)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 160)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 200)          40240900    input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1)            0           sequential_3[1][0]               \n",
      "                                                                 sequential_3[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            2           dot_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 40,240,902\n",
      "Trainable params: 40,240,902\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dual_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = './saved_wt.h5'\n",
    "dual_encoder.save_weights(weight_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "test_xc_textline = test_reader()\n",
    "test_xr_textline = test_reader()\n",
    "test_xc_textline.init_reader(K.get_session())\n",
    "test_xr_textline.init_reader(K.get_session())\n",
    "K.get_session().run(test_xc_textline.epoch_input().initializer)\n",
    "K.get_session().run(test_xr_textline.epoch_input().initializer)\n",
    "test_c = test_xc_textline.epoch_input().get_next()['Context']\n",
    "test_r = test_xr_textline.epoch_input().get_next()['GroundTruth']\n",
    "\n",
    "\n",
    "train_xc_textline = reader()\n",
    "train_xr_textline = reader()\n",
    "train_xl_textline = reader()\n",
    "train_xc_textline.init_reader(K.get_session())\n",
    "train_xr_textline.init_reader(K.get_session())\n",
    "train_xl_textline.init_reader(K.get_session())\n",
    "K.get_session().run(train_xc_textline.epoch_input().initializer)\n",
    "K.get_session().run(train_xr_textline.epoch_input().initializer)\n",
    "K.get_session().run(train_xl_textline.epoch_input().initializer)\n",
    "train_c = train_xc_textline.epoch_input().get_next()['Context']\n",
    "train_r = train_xr_textline.epoch_input().get_next()['Utterance']\n",
    "train_l = train_xl_textline.epoch_input().get_next()['Label']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(key):\n",
    "    K.clear_session()\n",
    "    test_xc_textline = test_reader()\n",
    "    test_xr_textline = test_reader()\n",
    "    test_xc_textline.init_reader(K.get_session())\n",
    "    test_xr_textline.init_reader(K.get_session())\n",
    "    K.get_session().run(test_xc_textline.epoch_input().initializer)\n",
    "    K.get_session().run(test_xr_textline.epoch_input().initializer)\n",
    "    test_c = test_xc_textline.epoch_input().get_next()['Context']\n",
    "    test_r = test_xr_textline.epoch_input().get_next()[key]\n",
    "    \n",
    "    \n",
    "    test_encoder = Sequential()\n",
    "    embeddin_layer = Embedding(output_dim=100,\n",
    "                            input_dim=MAX_NB_WORDS+1,\n",
    "                            input_length=160,\n",
    "                            weights=[embedding_matrix],\n",
    "                            mask_zero=True,\n",
    "                            trainable=True)\n",
    "    lstm_layer = LSTM(units=200)\n",
    "    test_encoder.add(embeddin_layer)\n",
    "    test_encoder.add(lstm_layer)\n",
    "\n",
    "    test_context_input = Input(shape=(160,),tensor=test_c)\n",
    "    test_response_input = Input(shape=(160,),tensor=test_r)\n",
    "    # context_input = Input(shape=(160,), dtype='float32')\n",
    "    # response_input = Input(shape=(160,), dtype='float32')\n",
    "\n",
    "    test_context_branch = test_encoder(test_context_input)\n",
    "    test_response_branch = test_encoder(test_response_input)\n",
    "\n",
    "    test_concatenated = keras.layers.Dot(axes=1)(\n",
    "                                [test_context_branch, \n",
    "                                 test_response_branch])\n",
    "    test_out = Dense((1), activation = \"sigmoid\") (test_concatenated)\n",
    "    test_dual_encoder = Model([test_context_input, test_response_input], test_out)\n",
    "    test_dual_encoder.load_weights(weight_path)\n",
    "    test_dual_encoder.compile(loss='binary_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "#     test_dual_encoder.summary()\n",
    "\n",
    "    his = test_dual_encoder.predict(x=None,steps=100)\n",
    "    return his\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D0\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D1\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D2\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D3\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D4\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D5\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D6\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D7\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n",
      "D8\n",
      "word_dict size: 434055\n",
      "word_dict size: 434055\n"
     ]
    }
   ],
   "source": [
    "key_list =  ['GroundTruth','D0','D1','D2','D3','D4','D5','D6','D7','D8']\n",
    "res = []\n",
    "for key in key_list:\n",
    "    print(key)\n",
    "    res.append(test_model(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51234375\n"
     ]
    }
   ],
   "source": [
    "res = np.array(res)\n",
    "count = 0\n",
    "for i in range(len(res[0])):\n",
    "    if 0 in res[:,i].reshape(10).argsort()[-2:]:\n",
    "        count+=1\n",
    "print(count/(512*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7787639  0.812789   0.21565938 0.42614526 0.23397344 0.7867426\n",
      " 0.58415186 0.5378984  0.39631486 0.39908376]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.39908376], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(res[:,0].reshape(10))\n",
    "res[:,0].reshape(10)[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a ={}\n",
    "a['bb'] = 1\n",
    "a['cc'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,d = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-085b32debabd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'C' is not defined"
     ]
    }
   ],
   "source": [
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
